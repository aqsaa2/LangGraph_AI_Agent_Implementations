
# LangGraph RAG Chatbot

This project implements a **Retrieval-Augmented Generation (RAG)** chatbot using **LangGraph**, **LangChain**, and **OpenAI**. It supports conversational memory and document-based retrieval using a PDF, allowing natural language querying over documents.

---

## ğŸš€ Features

- âœ… Conversational chatbot with memory (via `ConversationBufferMemory`)
- âœ… PDF ingestion using LangChain's `PyPDFLoader`
- âœ… Vector-based retrieval using `FAISS`
- âœ… Custom RAG flow using LangGraph's stateful nodes and graph-based control
- âœ… Integration with OpenAI's GPT models (GPT-4, GPT-3.5)

---

## ğŸ› ï¸ Technologies Used

- Python 3.10+
- [LangGraph](https://github.com/langchain-ai/langgraph)
- [LangChain](https://github.com/langchain-ai/langchain)
- FAISS for vector similarity search
- OpenAI (for LLM)
- tiktoken (for token counting)

---

## ğŸ“ Folder Structure

```
langgraph_chatbot/
â”œâ”€â”€ main.py             # Main file to run the chatbot
â”œâ”€â”€ state.py            # Defines the State object for LangGraph
â”œâ”€â”€ nodes.py            # Contains graph nodes for RAG and conversation
â”œâ”€â”€ graph.py            # Builds and compiles the LangGraph graph
â”œâ”€â”€ ingest.py           # Loads PDF, chunks it, and creates FAISS retriever
â”œâ”€â”€ utils.py            # Utility functions for setup and environment
â”œâ”€â”€ README.md           # This file
â””â”€â”€ .env                # Contains API keys (not included in repo)
```

---

## ğŸ“¦ Installation

1. **Clone the repository**

```bash
git clone https://github.com/yourusername/langgraph_chatbot.git
cd langgraph_chatbot
```

2. **Create a virtual environment (optional but recommended)**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install the dependencies**

```bash
pip install -r requirements.txt
```

> You can also install manually:
```bash
pip install langchain langgraph openai faiss-cpu tiktoken python-dotenv
```

---

## ğŸ”‘ Environment Variables

Create a `.env` file in the project root with the following:

```
OPENAI_API_KEY=your_openai_api_key
```

---

## ğŸ“„ Add Your Document

Place your PDF file in the root directory (e.g., `sample.pdf`), or change the path inside `ingest.py`.

---

## â–¶ï¸ Running the Chatbot

To run the chatbot:

```bash
python main.py
```

Youâ€™ll enter a command-line interface (CLI) where you can chat with your document-based assistant.

---

## ğŸ§  Key Implementation Details

- **Graph-based Flow**: LangGraph lets us build a directed graph of nodes:
  - `ingest.py`: Loads and processes the PDF.
  - `nodes.py`: Defines two nodes:
    - `retrieve_docs`: Uses the retriever to fetch top-k relevant chunks.
    - `call_rag_chain`: Constructs the prompt and calls OpenAI GPT model.
  - `graph.py`: Builds the flow:
    ```
    user_input â†’ retrieve_docs â†’ call_rag_chain â†’ output
    ```
- **Memory**: Uses `ConversationBufferMemory` to maintain context between user messages.
- **Document Ingestion**:
  - Splits PDF into ~800 token chunks with 50-token overlap.
  - Embeds chunks using OpenAI Embeddings.
  - Stores vectors in FAISS index.

---

## ğŸ§© Optional Enhancements

- ğŸ”§ Add Streamlit or Gradio frontend
- ğŸ§  Replace `ConversationBufferMemory` with `ConversationSummaryBufferMemory` for long chats
- ğŸ’¾ Save and load vectorstore from disk
- ğŸ“Š Add analytics like token usage or response times
- ğŸ“š Allow multiple document ingestion

---

## ğŸ“¬ Contact

For support or questions, open an issue or contact the maintainer at [your-email@example.com].

---

## ğŸ“œ License

This project is licensed under the MIT License.
